{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c9746c2-55d9-4d2d-b651-88072255591f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.training.example import Example\n",
    "from PyPDF2 import PdfReader\n",
    "from pptx import Presentation\n",
    "from docx import Document\n",
    "from collections import Counter\n",
    "person_tagger = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "#Para detector de plagio\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "#Para web:\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import fnmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb0fcdd7-62c7-4900-873d-5d99a327568f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#.docx .pptx .pdf\n",
    "def obtener_nombre_archivos(extension):\n",
    "    archivos = []\n",
    "    for archivo in os.listdir(\"./dataset\"):\n",
    "        if archivo.endswith(extension):\n",
    "            archivos.append(archivo)\n",
    "    return archivos\n",
    "\n",
    "def leer_pdf(ruta_archivo):\n",
    "    archivoLectura = open(ruta_archivo,'rb')\n",
    "    documento = PdfReader(archivoLectura)\n",
    "    texto_documento = \"\"\n",
    "\n",
    "    for pagina in documento.pages:\n",
    "        texto_documento += pagina.extract_text()\n",
    "\n",
    "    texto_limpio = re.sub(r'[^a-zA-Z0-9.,áéíóúÁÉÍÓÚñÑ]+', ' ', texto_documento)\n",
    "    return texto_limpio\n",
    "\n",
    "def leer_pptx(filepath):\n",
    "    presentation = Presentation(filepath)\n",
    "    texto_documento = \"\"\n",
    "\n",
    "    # Recorrer todas las diapositivas\n",
    "    for slide in presentation.slides:\n",
    "        # Recorrer todos los elementos de texto en la diapositiva\n",
    "        for shape in slide.shapes:\n",
    "            if shape.has_text_frame:\n",
    "                for paragraph in shape.text_frame.paragraphs:\n",
    "                    for run in paragraph.runs:\n",
    "                        texto_documento += run.text\n",
    "                        texto_documento += \" \"\n",
    "                        \n",
    "    texto_limpio = re.sub(r'[^a-zA-Z0-9.,áéíóúÁÉÍÓÚñÑ]+', ' ', texto_documento)\n",
    "    return texto_limpio\n",
    "\n",
    "def leer_docx(ruta_archivo):\n",
    "    doc = Document(ruta_archivo)\n",
    "    texto_documento = \"\"\n",
    "\n",
    "    # Recorrer todos los párrafos del documento\n",
    "    for paragraph in doc.paragraphs:\n",
    "        texto_documento += paragraph.text\n",
    "        texto_documento += \" \"\n",
    "\n",
    "    texto_limpio = re.sub(r'[^a-zA-Z0-9.,áéíóúÁÉÍÓÚñÑ]+', ' ', texto_documento)\n",
    "    return texto_limpio\n",
    "\n",
    "def leer_web(link):\n",
    "    soup = BeautifulSoup(link,'html.parser')\n",
    "    buscarContenido = soup.find('body')\n",
    "    parrafos = buscarContenido.find_all('p')\n",
    "    \n",
    "    listaStr = list()\n",
    "    for elemento in parrafos:\n",
    "        parrafosUrl = elemento.getText()\n",
    "        texto_limpio = re.sub(r'[^a-zA-Z0-9.,áéíóúÁÉÍÓÚñÑ]+', ' ', parrafosUrl)\n",
    "        listaStr.append(texto_limpio)\n",
    "    \n",
    "    texto_concatenado = ' '.join(listaStr)\n",
    "    return texto_concatenado\n",
    "\n",
    "def leer_documentos(nombre_archivos):\n",
    "    archivos = []\n",
    "    for archivo in nombre_archivos:\n",
    "        if(archivo.endswith(\".pdf\")):\n",
    "            archivos.append(leer_pdf(\"./dataset/\" + archivo))\n",
    "        if(archivo.endswith(\".pptx\")):\n",
    "            archivos.append(leer_pptx(\"./dataset/\" + archivo))\n",
    "        if(archivo.endswith(\".docx\")):\n",
    "            archivos.append(leer_docx(\"./dataset/\" + archivo))\n",
    "    return archivos\n",
    "\n",
    "def leer_archivo_analizar(nombre_archivo):\n",
    "    if(nombre_archivo.endswith(\".pdf\")):\n",
    "            return leer_pdf(nombre_archivo)\n",
    "    if(nombre_archivo.endswith(\".pptx\")):\n",
    "            return leer_pptx(nombre_archivo)\n",
    "    if(nombre_archivo.endswith(\".docx\")):\n",
    "            return leer_docx(nombre_archivo)\n",
    "    if(nombre_archivo.startswith(\"https://\")):\n",
    "            return leer_web(nombre_archivo)\n",
    "\n",
    "def obtener_nombre_archivo(ruta):\n",
    "    nombre_archivo = os.path.basename(ruta)\n",
    "    return nombre_archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13065889-f941-4b82-8f9a-cd40c2722ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecución\n",
    "nombres_archivos_pdf = obtener_nombre_archivos(\".pdf\")\n",
    "nombres_archivos_docx = obtener_nombre_archivos(\".docx\")\n",
    "nombres_archivos_doc = obtener_nombre_archivos(\".doc\")\n",
    "nombres_archivos_pptx = obtener_nombre_archivos(\".pptx\")\n",
    "nombre_archivos = []\n",
    "nombre_archivos.extend(nombres_archivos_pdf)\n",
    "nombre_archivos.extend(nombres_archivos_docx)\n",
    "nombre_archivos.extend(nombres_archivos_doc)\n",
    "nombre_archivos.extend(nombres_archivos_pptx)\n",
    "corpus = leer_documentos(nombre_archivos)\n",
    "nombre_archivo_a_analizar = \"./documentoSospechoso.pdf\"\n",
    "documento_a_analizar = leer_archivo_analizar(nombre_archivo_a_analizar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f00bc872-d841-407c-a297-7f48ada986c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#documento_a_analizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e94214e-ea20-4119-ba62-ae8c3c1afa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre del Archivo Procesado: documentoSospechoso.pdf\n"
     ]
    }
   ],
   "source": [
    "print(\"Nombre del Archivo Procesado: \" + nombre_archivo_a_analizar[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30494bb3-093e-42c2-9773-d302b5a0907e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities [('Alumno Pedro', 'MISC')]\n"
     ]
    }
   ],
   "source": [
    "#Entrenamiento\n",
    "#AIEngineering - Custom Named Entity Recognition using Python\n",
    "entrenamiento = [\n",
    "    (\"Alumno Pedro picazo\",{\"entities\": [(0,6,\"IDENTIFICADOR\")]})\n",
    "]\n",
    "\n",
    "ner = person_tagger.get_pipe(\"ner\")\n",
    "\n",
    "for _, anotaciones in entrenamiento:\n",
    "    for ent in anotaciones.get(\"entities\"):\n",
    "        ner.add_label(ent[2])\n",
    "\n",
    "#disable_pipes = [pipe for pipe in person_tagger.pipe_names if pipe != 'ner']\n",
    "#with person_tagger.disable_pipes(*disable_pipes):\n",
    "optimizer = person_tagger.resume_training()\n",
    "    \n",
    "for iteration in range(5):\n",
    "       random.shuffle(entrenamiento)\n",
    "       losses = {}\n",
    "        \n",
    "       batches = minibatch(entrenamiento,size=compounding(3.8,4.0,1.001))\n",
    "       for batch in batches:\n",
    "            examples = []\n",
    "            for text, annotation in batch:\n",
    "                examples.append(Example.from_dict(person_tagger.make_doc(text), annotation))\n",
    "            person_tagger.update(examples, drop=0.5, losses=losses, sgd=optimizer)\n",
    "\n",
    "\n",
    "for texto, _ in entrenamiento:\n",
    "    documento = person_tagger(texto)\n",
    "    print('Entities', [(ent.text, ent.label_) for ent in documento.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d05dd4e-ee35-4f44-bc65-d7e57b0a2271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posibles nombres del Autor:\n"
     ]
    }
   ],
   "source": [
    "print(\"Posibles nombres del Autor:\")\n",
    "documento_a_analizar_clean = re.sub(r'\\W+', ' ', documento_a_analizar)\n",
    "tokens = word_tokenize(documento_a_analizar_clean)\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "tokens_documento = [token.lower() for token in tokens if token.lower() not in stop_words]\n",
    "per_tagger = person_tagger(documento_a_analizar_clean)\n",
    "\n",
    "def contains_word(text, word):\n",
    "    return word in text\n",
    "\n",
    "for autor in per_tagger.ents[0:12]:\n",
    "    token_aux = word_tokenize(autor.text)\n",
    "    if(autor.label_ == \"PER\" and len(token_aux) > 2):\n",
    "        if(contains_word(autor.text,\"Alumno\")):\n",
    "            index = autor.text.find(\"Alumno\")\n",
    "            nuevo_texto = autor.text[index:]\n",
    "            nuevo_texto = nuevo_texto.replace(\"Alumno\",\"\")\n",
    "            print(nuevo_texto)\n",
    "        else:\n",
    "            print(autor.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc355981-0c25-4b9e-b03c-6ed6ced68cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_potential_plagiarized_sentences(documents, new_document):\n",
    "    model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
    "    sentences = nltk.sent_tokenize(new_document)\n",
    "    sentencesD = nltk.sent_tokenize(documents[0])\n",
    "\n",
    "    # Embeddings de los documentos existentes\n",
    "    document_embeddings = model.encode(sentencesD, convert_to_tensor=True)\n",
    "\n",
    "    # Embeddings de las frases del nuevo documento\n",
    "    new_embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "    # Calcular la similitud de coseno entre los embeddings\n",
    "    cos_sim_scores = util.pytorch_cos_sim(new_embeddings, document_embeddings)\n",
    "\n",
    "    potential_plagiarized_sentences = []\n",
    "    plagiarized_lines = []  # Lista para almacenar las líneas plagiadas del documento\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        max_sim_score = max(cos_sim_scores[i])\n",
    "        if max_sim_score > 0.6:  # Ajusta el umbral según sea necesario\n",
    "            doc_index = cos_sim_scores[i].argmax().item()\n",
    "            plagiarism_percentage = max_sim_score * 100\n",
    "            potential_plagiarized_sentences.append((sentence, doc_index, i, plagiarism_percentage))\n",
    "\n",
    "            if doc_index < len(sentencesD):\n",
    "                plagiarized_lines.append(sentencesD[doc_index])  # Agregar la línea plagiada del documento\n",
    "            else:\n",
    "                plagiarized_lines.append(\"\")  # Agregar una cadena vacía si el índice está fuera de rango\n",
    "\n",
    "    return potential_plagiarized_sentences, plagiarized_lines\n",
    "\n",
    "\n",
    "for index in range(len(corpus)):\n",
    "    similitud = SequenceMatcher(None, documento_a_analizar, corpus[index]).ratio() * 100\n",
    "    if similitud > 101:\n",
    "        print(\"Documento analizado: \" + nombre_archivos[index])\n",
    "        print(f\"Porcentaje de potencial plagio: {similitud:.2f}%\")\n",
    "        potential_plagiarized_sentences, plagiarized_lines = find_potential_plagiarized_sentences([corpus[index]], documento_a_analizar)\n",
    "        for sentence, doc_index, sent_index, plagiarism_percentage in potential_plagiarized_sentences:\n",
    "            print(f\"Frase: '{sentence}', Ubicación: {sent_index}\")\n",
    "            if doc_index < len(plagiarized_lines):\n",
    "                print(f\"PFPDC:  {plagiarized_lines[doc_index]}\")\n",
    "            else:\n",
    "                print(\"No se encontró la línea correspondiente.\")\n",
    "        print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6997a50-3c3a-4546-ac5f-674280055d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport spacy\\n\\ndef identify_main_topic(text):\\n    # Load the Spanish language model in spaCy\\n    nlp = spacy.load(\"es_core_news_sm\")\\n\\n    # Process the text\\n    doc = nlp(text)\\n\\n    # Extract the main topic from the text\\n    main_topic = None\\n\\n    for sentence in doc.sents:\\n        # Iterate through each sentence in the document\\n\\n        # Check if the sentence contains a noun phrase\\n        if any(token.pos_ == \"NOUN\" for token in sentence):\\n            main_topic = sentence.text\\n            break\\n\\n    return main_topic\\n\\n# Example usage\\ntext = \"Márketing en internet y nueva economía Economía de experiencia Profesores Dr. Alejandro Prince Ing. Hernán Borré Ing. Maximiliano Bracho Alumno Gallazzi, Pablo Gabriel 143.370 2 Fecha de Presentación 10 04 2017 Cuestionario 1. Qué 3 elementos hacen resurgir con fuerza la idea de una economía de experiencia...\"\\ntopic = identify_main_topic(text)\\nprint(topic)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import spacy\n",
    "\n",
    "def identify_main_topic(text):\n",
    "    # Load the Spanish language model in spaCy\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "    # Process the text\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Extract the main topic from the text\n",
    "    main_topic = None\n",
    "\n",
    "    for sentence in doc.sents:\n",
    "        # Iterate through each sentence in the document\n",
    "\n",
    "        # Check if the sentence contains a noun phrase\n",
    "        if any(token.pos_ == \"NOUN\" for token in sentence):\n",
    "            main_topic = sentence.text\n",
    "            break\n",
    "\n",
    "    return main_topic\n",
    "\n",
    "# Example usage\n",
    "text = \"Márketing en internet y nueva economía Economía de experiencia Profesores Dr. Alejandro Prince Ing. Hernán Borré Ing. Maximiliano Bracho Alumno Gallazzi, Pablo Gabriel 143.370 2 Fecha de Presentación 10 04 2017 Cuestionario 1. Qué 3 elementos hacen resurgir con fuerza la idea de una economía de experiencia...\"\n",
    "topic = identify_main_topic(text)\n",
    "print(topic)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89953490-0ccf-4b67-8d6d-d96f262b9a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Márketing\n",
      "en\n",
      "internet\n",
      "y\n",
      "nueva\n",
      "economía\n",
      "Economía\n",
      "de\n",
      "experiencia\n"
     ]
    }
   ],
   "source": [
    "def lemmatization(texts, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "    nlp = spacy.load(\"es_core_news_sm\", disable=[\"parser\", \"ner\"])\n",
    "    texts_out = []\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        new_text = []\n",
    "        for token in doc:\n",
    "            if token.pos_ in allowed_postags:\n",
    "                new_text.append(token.lemma_)\n",
    "        final = \" \".join(new_text)\n",
    "        texts_out.append(final)\n",
    "    return (texts_out)\n",
    "\n",
    "\n",
    "lemmatized_texts = lemmatization([documento_a_analizar])\n",
    "tokensk = word_tokenize(documento_a_analizar_clean)\n",
    "for token in tokens:\n",
    "    if token in [\"Profesor\", \"Profesores\", \"Alumno\", \"Alumnos\", \"Ing\"]:\n",
    "        break\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b83854ee-e81d-4dfa-96b5-8fb83b1fc26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temas principales:\n",
      "\n",
      "personalización\n",
      "cliente\n",
      "experiencia\n",
      "producto\n",
      "economía\n",
      "dimensiones\n",
      "masiva\n",
      "empresa\n",
      "hablamos\n",
      "referimos\n"
     ]
    }
   ],
   "source": [
    "#Investigar sobre LDA\n",
    "def identificar_tema(texto):\n",
    "    # Cargar el modelo en español de spaCy\n",
    "    nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "    # Procesar el texto con spaCy\n",
    "    doc = nlp(texto)\n",
    "\n",
    "    # Filtrar palabras irrelevantes y puntuaciones\n",
    "    palabras = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "    palabras = [re.sub(r'[0-9]', '', palabra) for palabra in palabras]\n",
    "\n",
    "    # Contar la frecuencia de las palabras\n",
    "    frecuencias = Counter(palabras)\n",
    "\n",
    "    # Obtener los 11 temas principales más frecuentes\n",
    "    temas_principales = [tema for tema, frecuencia in frecuencias.most_common(11)]\n",
    "\n",
    "    return temas_principales\n",
    "\n",
    "texto = \"El cambio climático es un problema global que requiere acciones urgentes.\"\n",
    "temas = identificar_tema(documento_a_analizar)\n",
    "print(\"Temas principales:\")\n",
    "for tema in temas:\n",
    "    print(tema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a722e65-e9af-4891-9d6c-7c803efa476a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Disclaimer: Este trabajo fue desarrollado en base a lo aprendido en la cursada de la materia de Procesamiento de Lenguaje Natural de la UTN frba,\\n    la misma fue dicatada por el profesor Mg. Ing. Hernán Borré\\n    \\n    Fuentes:\\n    \\n    (2023). Programación en Python. ChatGPT\\n    https://chat.openai.com/\\n    \\n    codebasics. (2023). Named Entity Recognition (NER): NLP Tutorial For Beginners - 12. Youtube.\\n    \\n    (2023). spaCy.\\n    https://spacy.io/\\n    \\n    \\n    Sitios web: Apellido, Inicial del nombre del autor (si está disponible). (Año de publicación o actualización). Título de la página. Nombre del sitio web. URL\\n    Ejemplo: González, J. (2023). Posicionamiento web en Google, factores SEO que más influyen en el posicionamiento en Google. As Posicionamiento Web. https://www.asposicionamientoweb.com/seo/factores-posicionamiento-web-seo-google/\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Disclaimer: Este trabajo fue desarrollado en base a lo aprendido en la cursada de la materia de Procesamiento de Lenguaje Natural de la UTN frba,\n",
    "    la misma fue dicatada por el profesor Mg. Ing. Hernán Borré (https://github.com/hernanborre)\n",
    "    \n",
    "    Fuentes:\n",
    "    (2023). Programación en Python. ChatGPT\n",
    "    https://chat.openai.com/\n",
    "    \n",
    "    codebasics. (2021). Named Entity Recognition (NER): NLP Tutorial For Beginners - 12. Youtube.\n",
    "    https://www.youtube.com/watch?v=2XUhKpH0p4M&list=PL6qsQ_bxK9ZznUs0WiDGhtOr6A_EAHWKS&index=2&t=1087s&ab_channel=codebasics\n",
    "    \n",
    "    fpread. (2020). scrapear para detectar plagios python. Youtube.\n",
    "    https://www.youtube.com/watch?v=OzgQ0QYxO20&list=PL6qsQ_bxK9ZznUs0WiDGhtOr6A_EAHWKS&index=4&ab_channel=fpred\n",
    "    \n",
    "    I know python (2021). Plagiarism detector using python. Youtube.\n",
    "    https://www.youtube.com/watch?v=lRzC3w2NDg0&list=PL6qsQ_bxK9ZznUs0WiDGhtOr6A_EAHWKS&index=1&ab_channel=Iknowpython\n",
    "    \n",
    "    (2023). collections — Container datatypes. Python Org.\n",
    "    https://docs.python.org/3/library/collections.html#collections.Counter\n",
    "    \n",
    "    (2023). spaCy.\n",
    "    https://spacy.io/\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a78231c-0401-4c5d-99ee-9ae7f60a4d00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-nlp] *",
   "language": "python",
   "name": "conda-env-.conda-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
