{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c9746c2-55d9-4d2d-b651-88072255591f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.training.example import Example\n",
    "from PyPDF2 import PdfReader\n",
    "from pptx import Presentation\n",
    "from docx import Document\n",
    "from collections import Counter\n",
    "person_tagger = spacy.load(\"es_core_news_sm\")\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "\n",
    "#Para detector de plagio\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "#Para web:\n",
    "from googlesearch import search\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import fnmatch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb0fcdd7-62c7-4900-873d-5d99a327568f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#.docx .pptx .pdf\n",
    "def obtener_nombre_archivos(extension):\n",
    "    archivos = []\n",
    "    for archivo in os.listdir(\"./dataset\"):\n",
    "        if archivo.endswith(extension):\n",
    "            archivos.append(archivo)\n",
    "    return archivos\n",
    "\n",
    "def leer_pdf(ruta_archivo):\n",
    "    archivoLectura = open(ruta_archivo,'rb')\n",
    "    documento = PdfReader(archivoLectura)\n",
    "    texto_documento = \"\"\n",
    "\n",
    "    for pagina in documento.pages:\n",
    "        texto_documento += pagina.extract_text()\n",
    "\n",
    "    texto_limpio = re.sub(r'[^a-zA-Z0-9.,áéíóúÁÉÍÓÚñÑ]+', ' ', texto_documento)\n",
    "    return texto_limpio\n",
    "\n",
    "def leer_pptx(filepath):\n",
    "    presentation = Presentation(filepath)\n",
    "    texto_documento = \"\"\n",
    "\n",
    "    # Recorrer todas las diapositivas\n",
    "    for slide in presentation.slides:\n",
    "        # Recorrer todos los elementos de texto en la diapositiva\n",
    "        for shape in slide.shapes:\n",
    "            if shape.has_text_frame:\n",
    "                for paragraph in shape.text_frame.paragraphs:\n",
    "                    for run in paragraph.runs:\n",
    "                        texto_documento += run.text\n",
    "                        texto_documento += \" \"\n",
    "                        \n",
    "    texto_limpio = re.sub(r'[^a-zA-Z0-9.,áéíóúÁÉÍÓÚñÑ]+', ' ', texto_documento)\n",
    "    return texto_limpio\n",
    "\n",
    "def leer_docx(ruta_archivo):\n",
    "    doc = Document(ruta_archivo)\n",
    "    texto_documento = \"\"\n",
    "\n",
    "    # Recorrer todos los párrafos del documento\n",
    "    for paragraph in doc.paragraphs:\n",
    "        texto_documento += paragraph.text\n",
    "        texto_documento += \" \"\n",
    "\n",
    "    texto_limpio = re.sub(r'[^a-zA-Z0-9.,áéíóúÁÉÍÓÚñÑ]+', ' ', texto_documento)\n",
    "    return texto_limpio\n",
    "\n",
    "def leer_doc(ruta_archivo):\n",
    "    with open(ruta_archivo, 'r', encoding='latin-1') as archivo:\n",
    "        texto_documento = archivo.read()\n",
    "        texto_filtrado = re.sub(r'[^a-zA-Z0-9.,áéíóúÁÉÍÓÚñÑ]+', ' ', texto_documento)\n",
    "        return texto_filtrado\n",
    "\n",
    "def leer_web(link):\n",
    "    try:\n",
    "        pagina = requests.get(link).text\n",
    "        soup = BeautifulSoup(pagina, 'html.parser')\n",
    "        buscarContenido = soup.find('body')\n",
    "        \n",
    "        if buscarContenido:\n",
    "            parrafos = buscarContenido.find_all('p')\n",
    "            listaStr = []\n",
    "            \n",
    "            for elemento in parrafos:\n",
    "                parrafo = elemento.get_text(strip=True)\n",
    "                texto_limpio = re.sub(r'[^a-zA-Z0-9.,áéíóúÁÉÍÓÚñÑ\\s]+', ' ', parrafo)\n",
    "                listaStr.append(texto_limpio)\n",
    "            \n",
    "            texto_concatenado = ' '.join(listaStr)\n",
    "            return texto_concatenado\n",
    "        else:\n",
    "            print(\"No se encontró contenido en la página.\")\n",
    "            return \"\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error al acceder a la página:\", e)\n",
    "        return \"\"\n",
    "\n",
    "def leer_documentos(nombre_archivos):\n",
    "    archivos = []\n",
    "    for archivo in nombre_archivos:\n",
    "        if(archivo.endswith(\".pdf\")):\n",
    "            archivos.append(leer_pdf(\"./dataset/\" + archivo))\n",
    "        if(archivo.endswith(\".pptx\")):\n",
    "            archivos.append(leer_pptx(\"./dataset/\" + archivo))\n",
    "        if(archivo.endswith(\".docx\")):\n",
    "            archivos.append(leer_docx(\"./dataset/\" + archivo))\n",
    "    return archivos\n",
    "\n",
    "def leer_archivo_analizar(nombre_archivo):\n",
    "    if(nombre_archivo.endswith(\".pdf\")):\n",
    "            return leer_pdf(nombre_archivo)\n",
    "    if(nombre_archivo.endswith(\".pptx\")):\n",
    "            return leer_pptx(nombre_archivo)\n",
    "    if(nombre_archivo.endswith(\".docx\")):\n",
    "            return leer_docx(nombre_archivo)\n",
    "    if(nombre_archivo.startswith(\"https://\")):\n",
    "            return leer_web(nombre_archivo)\n",
    "        \n",
    "def encontrar_titulo_pagina(url):\n",
    "    if url is None:\n",
    "        print(\"No se encontró contenido en la página.\")\n",
    "    else:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        titulo_tag = soup.title\n",
    "        if titulo_tag is not None:\n",
    "            titulo = titulo_tag.string\n",
    "            return titulo\n",
    "        else:\n",
    "            print(\"No se encontró etiqueta <title> en la página.\")\n",
    "    return None\n",
    "\n",
    "def obtener_nombre_archivo(ruta):\n",
    "    if(ruta.startswith(\"https://\")):\n",
    "        return encontrar_titulo_pagina(ruta)\n",
    "    nombre_archivo = os.path.basename(ruta)\n",
    "    return nombre_archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13065889-f941-4b82-8f9a-cd40c2722ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecución\n",
    "nombres_archivos_pdf = obtener_nombre_archivos(\".pdf\")\n",
    "nombres_archivos_docx = obtener_nombre_archivos(\".docx\")\n",
    "#nombres_archivos_doc = obtener_nombre_archivos(\".doc\")\n",
    "nombres_archivos_pptx = obtener_nombre_archivos(\".pptx\")\n",
    "nombre_archivos = []\n",
    "nombre_archivos.extend(nombres_archivos_pdf)\n",
    "nombre_archivos.extend(nombres_archivos_docx)\n",
    "#nombre_archivos.extend(nombres_archivos_doc)\n",
    "nombre_archivos.extend(nombres_archivos_pptx)\n",
    "corpus = leer_documentos(nombre_archivos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e94214e-ea20-4119-ba62-ae8c3c1afa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre del Archivo Procesado: Economía de experiencia.pdf\n"
     ]
    }
   ],
   "source": [
    "#nombre_archivo_a_analizar = \"https://carlos-vasquez11.github.io/pagina-nlp/\"\n",
    "#nombre_archivo_a_analizar = \"https://as.com/meristation/2019/05/26/noticias/1558879918_863987.html\"\n",
    "nombre_archivo_a_analizar = \"./Economía de experiencia.pdf\"\n",
    "documento_a_analizar = leer_archivo_analizar(nombre_archivo_a_analizar)\n",
    "documento_a_analizar_clean = re.sub(r'\\W+', ' ', documento_a_analizar)\n",
    "print(\"Nombre del Archivo Procesado: \" + obtener_nombre_archivo(nombre_archivo_a_analizar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30494bb3-093e-42c2-9773-d302b5a0907e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities [('Alumno Pedro', 'PER')]\n"
     ]
    }
   ],
   "source": [
    "#Entrenamiento\n",
    "entrenamiento = [\n",
    "    (\"Alumno Pedro picazo\",{\"entities\": [(0,6,\"IDENTIFICADOR\")]})\n",
    "]\n",
    "\n",
    "ner = person_tagger.get_pipe(\"ner\")\n",
    "\n",
    "for _, anotaciones in entrenamiento:\n",
    "    for ent in anotaciones.get(\"entities\"):\n",
    "        ner.add_label(ent[2])\n",
    "\n",
    "#disable_pipes = [pipe for pipe in person_tagger.pipe_names if pipe != 'ner']\n",
    "#with person_tagger.disable_pipes(*disable_pipes):\n",
    "optimizer = person_tagger.resume_training()\n",
    "    \n",
    "for iteration in range(5):\n",
    "    random.shuffle(entrenamiento)\n",
    "    losses = {}\n",
    "        \n",
    "    batches = minibatch(entrenamiento,size=compounding(3.8,4.0,1.001))\n",
    "    for batch in batches:\n",
    "        examples = []\n",
    "        for text, annotation in batch:\n",
    "            examples.append(Example.from_dict(person_tagger.make_doc(text), annotation))\n",
    "        person_tagger.update(examples, drop=0.5, losses=losses, sgd=optimizer)\n",
    "\n",
    "#Ing Hernán Borré Estudiante Juanquin Perez | MISC o PER\n",
    "            \n",
    "for texto, _ in entrenamiento:\n",
    "    documento = person_tagger(texto)\n",
    "    print('Entities', [(ent.text, ent.label_) for ent in documento.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d05dd4e-ee35-4f44-bc65-d7e57b0a2271",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Posibles nombres del Autor:\")\n",
    "tokens = word_tokenize(documento_a_analizar_clean)\n",
    "tokens_documento = [token.lower() for token in tokens if token.lower() not in stop_words]\n",
    "per_tagger = person_tagger(documento_a_analizar_clean)\n",
    "\n",
    "def contains_word(text, word):\n",
    "    return word in text\n",
    "\n",
    "for autor in per_tagger.ents[0:20]:\n",
    "    token_aux = word_tokenize(autor.text)\n",
    "    if(autor.label_ == \"PER\" and len(token_aux) > 2):\n",
    "        if(contains_word(autor.text,\"Alumno\")):\n",
    "            index = autor.text.find(\"Alumno\")\n",
    "            nuevo_texto = autor.text[index:]\n",
    "            nuevo_texto = nuevo_texto.replace(\"Alumno\",\"\")\n",
    "            print(nuevo_texto)\n",
    "        else:\n",
    "            print(autor.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89953490-0ccf-4b67-8d6d-d96f262b9a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "márketing en internet y nueva economía economía de experiencia\n"
     ]
    }
   ],
   "source": [
    "def eliminar_ultima_palabra(cadena):\n",
    "    palabras = cadena.split()\n",
    "    palabras_sin_ultima = palabras[:-1]\n",
    "    nueva_cadena = \" \".join(palabras_sin_ultima)\n",
    "    return nueva_cadena\n",
    "\n",
    "def obtener_tema_documento(text, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "    doc = nlp(text)\n",
    "    new_text = []\n",
    "    propn = 0\n",
    "    final = \"\"\n",
    "    final_verdadero = \"\"\n",
    "    while len(word_tokenize(final_verdadero)) < 2:\n",
    "        for token in doc:\n",
    "            if token.pos_ == \"PROPN\":\n",
    "                propn += 1\n",
    "            else:\n",
    "                propn = 0\n",
    "            if(propn == 2):\n",
    "                break;\n",
    "            new_text.append(token.lower_)\n",
    "        final = \" \".join(new_text)\n",
    "        final_verdadero = eliminar_ultima_palabra(final)\n",
    "    \n",
    "    return final_verdadero\n",
    "\n",
    "temaPrincipal = \"\"\n",
    "\n",
    "temaPrincipal = obtener_tema_documento(documento_a_analizar)\n",
    "\n",
    "print(temaPrincipal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b83854ee-e81d-4dfa-96b5-8fb83b1fc26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temas principales:\n",
      "experiencia\n",
      "personalización\n",
      "producto\n",
      "economía\n",
      "dimensión\n",
      "ejemplo\n",
      "masivo\n",
      "empresa\n",
      "hablar\n",
      "referir\n"
     ]
    }
   ],
   "source": [
    "#Investigar sobre LDA\n",
    "def lematizacion(text, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "    nlp = spacy.load(\"es_core_news_sm\", disable=[\"parser\", \"ner\"])\n",
    "    doc = nlp(text)\n",
    "    new_text = []\n",
    "    for token in doc:\n",
    "        if token.pos_ in allowed_postags:\n",
    "            new_text.append(token.lemma_)\n",
    "    final = \" \".join(new_text)\n",
    "    return final\n",
    "\n",
    "def identificar_tema(texto):\n",
    "    # Procesar el texto con spaCy\n",
    "    doc = person_tagger(texto)\n",
    "\n",
    "    # Filtrar palabras irrelevantes, puntuaciones y números\n",
    "    palabras = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "    palabras = [re.sub(r'[0-9]', '', palabra) for palabra in palabras]\n",
    "\n",
    "    # Contar la frecuencia de las palabras\n",
    "    frecuencias = Counter(palabras)\n",
    "\n",
    "    # Obtengo las 11 palabras más frecuentes\n",
    "    temas_principales = [tema for tema, frecuencia in frecuencias.most_common(11)]\n",
    "\n",
    "    #La primera posición siempre es el espacio, es mejor no retornarlo\n",
    "    return temas_principales[1:]\n",
    "\n",
    "texo_lematizado = lematizacion(documento_a_analizar)\n",
    "temas = identificar_tema(texo_lematizado)\n",
    "print(\"Temas principales:\")\n",
    "for tema in temas:\n",
    "    print(tema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a78231c-0401-4c5d-99ee-9ae7f60a4d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.researchgate.net/publication/330741205_Mercadeo_Digital_en_la_nueva_economia\n",
      "https://mercado.com.ar/management-marketing/las-empresas-y-la-nueva-economia-digital-globalizada/\n",
      "No se encontró contenido en la página.\n",
      "No se encontró etiqueta <title> en la página.\n",
      "https://www.utnianos.com.ar/foro/tema-aporte-marketing-en-internet-y-nueva-econom%C3%ADa-tps\n",
      "https://www.academia.edu/38254540/Mercadeo_Digital_en_la_nueva_econom%C3%ADa\n",
      "https://contenttu.com/blog/inbound-marketing/google-marketing-digital-como-salvavidas-en-la-economia-2021\n",
      "https://www.redalyc.org/journal/3235/323549941003/html/\n",
      "https://academia.crandi.com/ventas-online/economia-de-la-experiencia/\n",
      "No se encontró contenido en la página.\n",
      "No se encontró etiqueta <title> en la página.\n"
     ]
    }
   ],
   "source": [
    "#Agrego fuentes de internet al corpus\n",
    "query = temaPrincipal\n",
    "#Por defecto nos retorna los primeros 10 resultados\n",
    "for link in search(query,num_results=5):\n",
    "    time.sleep(1)\n",
    "    if link is None:\n",
    "        print(\"No se encontró contenido en la página.\")\n",
    "    else:\n",
    "        print(link)\n",
    "        pagina = leer_web(link)\n",
    "        corpus.append(pagina)\n",
    "        nombre_archivos.append(encontrar_titulo_pagina(link)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc355981-0c25-4b9e-b03c-6ed6ced68cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_mayor(num1, num2):\n",
    "    if num1 > num2:\n",
    "        return num1\n",
    "    else:\n",
    "        return num2\n",
    "    \n",
    "max_porcentaje_plagio_documento = 0\n",
    "\n",
    "def parrafos_potencialmente_plagiados(documents, new_document):\n",
    "    model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
    "    sentences = nltk.sent_tokenize(new_document)\n",
    "    sentencesD = nltk.sent_tokenize(documents[0])\n",
    "\n",
    "    # Embeddings de los documentos existentes\n",
    "    document_embeddings = model.encode(sentencesD, convert_to_tensor=True)\n",
    "\n",
    "    # Embeddings de las frases del nuevo documento\n",
    "    new_embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "    # Calcular la similitud de coseno entre los embeddings\n",
    "    cos_sim_scores = util.pytorch_cos_sim(new_embeddings, document_embeddings)\n",
    "\n",
    "    potential_plagiarized_sentences = []\n",
    "    plagiarized_lines = []  # Lista para almacenar las líneas plagiadas del documento\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        max_sim_score = max(cos_sim_scores[i])\n",
    "        if max_sim_score > 0.95:  # Ajusta el umbral según sea necesario\n",
    "            doc_index = cos_sim_scores[i].argmax().item()\n",
    "            plagiarism_percentage = max_sim_score * 100\n",
    "            potential_plagiarized_sentences.append((sentence, doc_index, i, plagiarism_percentage))\n",
    "\n",
    "            if doc_index < len(sentencesD):\n",
    "                plagiarized_lines.append(sentencesD[doc_index])  # Agregar la línea plagiada del documento\n",
    "            else:\n",
    "                plagiarized_lines.append(\"\")  # Agregar una cadena vacía si el índice está fuera de rango\n",
    "\n",
    "    return potential_plagiarized_sentences, plagiarized_lines\n",
    "\n",
    "frases_plagiadas = set()\n",
    "\n",
    "for index in range(len(corpus)):\n",
    "    similitud = SequenceMatcher(None, corpus[index], documento_a_analizar).ratio() * 100\n",
    "    if similitud > 15:\n",
    "        max_porcentaje_plagio_documento = obtener_mayor(max_porcentaje_plagio_documento, similitud)\n",
    "        print(\"Documento analizado: \" + nombre_archivos[index])\n",
    "        print(f\"Porcentaje de potencial plagio: {similitud:.2f}%\")\n",
    "        potential_plagiarized_sentences, plagiarized_lines = parrafos_potencialmente_plagiados([corpus[index]], documento_a_analizar)\n",
    "        for sentence, doc_index, sent_index, plagiarism_percentage in potential_plagiarized_sentences:\n",
    "            print(f\"Frase: '{sentence}', Ubicación: {sent_index}\")\n",
    "            if doc_index < len(plagiarized_lines):\n",
    "                print(f\"PFPDC:  {plagiarized_lines[doc_index]}\")\n",
    "                frases_plagiadas.add(sent_index)\n",
    "            else:\n",
    "                print(\"No se encontró la línea correspondiente.\")\n",
    "        print(\" \")\n",
    "        \n",
    "##\n",
    "\n",
    "def contar_caracteres(lista_cadenas):\n",
    "    total_caracteres = 0\n",
    "    for cadena in lista_cadenas:\n",
    "        total_caracteres += len(cadena)\n",
    "    return total_caracteres\n",
    "\n",
    "def contar_caracteres_por_posicion(lista_cadenas, lista_indices):\n",
    "    total_caracteres = 0\n",
    "    for indice in lista_indices:\n",
    "        if indice < len(lista_cadenas):\n",
    "            total_caracteres += len(lista_cadenas[indice])\n",
    "    return total_caracteres\n",
    "        \n",
    "total_frases = nltk.sent_tokenize(documento_a_analizar)\n",
    "caracteres_totales = contar_caracteres(total_frases)\n",
    "caracteres_plagiados = contar_caracteres_por_posicion(total_frases,frases_plagiadas)\n",
    "porcentaje_plagio = caracteres_plagiados / caracteres_totales * 100\n",
    "\n",
    "if(max_porcentaje_plagio_documento > porcentaje_plagio):\n",
    "    print(f\"Porcentaje de plagio total aproximado: {max_porcentaje_plagio_documento:.2f}%\")\n",
    "else:\n",
    "    print(f\"Porcentaje de plagio total aproximado: {porcentaje_plagio:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a722e65-e9af-4891-9d6c-7c803efa476a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Disclaimer: Este trabajo fue desarrollado en base a lo aprendido en la cursada de la materia de Procesamiento de Lenguaje Natural de la UTN frba,\n",
    "    la misma fue dicatada por el profesor Mg. Ing. Hernán Borré (https://github.com/hernanborre)\n",
    "    \n",
    "    Fuentes:\n",
    "    (2023). Programación en Python. ChatGPT\n",
    "    https://chat.openai.com/\n",
    "    \n",
    "    AiEgineering. (2021). Custom Named Entity Recognition using Python\n",
    "    https://www.youtube.com/watch?v=1ePkOSGoIFI&t=855s&ab_channel=AIEngineering\n",
    "    \n",
    "    codebasics. (2021). Named Entity Recognition (NER): NLP Tutorial For Beginners - 12. Youtube.\n",
    "    https://www.youtube.com/watch?v=2XUhKpH0p4M&list=PL6qsQ_bxK9ZznUs0WiDGhtOr6A_EAHWKS&index=2&t=1087s&ab_channel=codebasics\n",
    "    \n",
    "    fpread. (2020). scrapear para detectar plagios python. Youtube.\n",
    "    https://www.youtube.com/watch?v=OzgQ0QYxO20&list=PL6qsQ_bxK9ZznUs0WiDGhtOr6A_EAHWKS&index=4&ab_channel=fpred\n",
    "    \n",
    "    I know python (2021). Plagiarism detector using python. Youtube.\n",
    "    https://www.youtube.com/watch?v=lRzC3w2NDg0&list=PL6qsQ_bxK9ZznUs0WiDGhtOr6A_EAHWKS&index=1&ab_channel=Iknowpython\n",
    "    \n",
    "    (2023). collections — Container datatypes. Python Org.\n",
    "    https://docs.python.org/3/library/collections.html#collections.Counter\n",
    "    \n",
    "    (2021). Python + Google: BÚSQUEDAS AUTOMATIZADAS: Cómo realizar búsquedas en Google usando Python. Youtube\n",
    "    https://www.youtube.com/watch?v=FE-WeaNRAeI&ab_channel=TruzzBlogg\n",
    "    \n",
    "    (2023). googlesearch. pypi.org\n",
    "    https://pypi.org/project/googlesearch-python/\n",
    "    \n",
    "    (2023). spaCy.\n",
    "    https://spacy.io/\n",
    "    \n",
    "    (2023). PyPDF2. pypi.org\n",
    "    https://pypi.org/project/PyPDF2/\n",
    "    \n",
    "    (2013) Read .doc file with python. StackOverflow\n",
    "    https://stackoverflow.com/questions/36001482/read-doc-file-with-python\n",
    "    \n",
    "    (2023). docx.\n",
    "    https://python-docx.readthedocs.io/en/latest/\n",
    "    \n",
    "    (2023). pptx.\n",
    "    https://python-pptx.readthedocs.io/en/latest/\n",
    "    \n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c3f3a3-83a3-4740-9c8f-adfe931d974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    \n",
    "import win32com.client\n",
    "\n",
    "word = win32com.client.Dispatch(\"Word.Application\")\n",
    "word.visible = False\n",
    "\n",
    "# for example, ``rel_path`` could be './myfile.doc'\n",
    "full_path = os.path.abspath('./dataset/Lopez Tomas - TP 6 - Sistemas Emergentes.doc')\n",
    "\n",
    "wb = word.Documents.Open(full_path)\n",
    "doc = word.ActiveDocument\n",
    "print(full_path)\n",
    "print(doc.Range())\n",
    "print(doc.Range().Text)\n",
    "\n",
    "# close the document\n",
    "doc.Close(False)\n",
    "\n",
    "# quit Word\n",
    "word.Quit()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1034a9-d585-4974-aae7-bdabaae42110",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documento_a_analizar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55c79be-628b-418c-b48c-48d5ac4b3c78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-nlp] *",
   "language": "python",
   "name": "conda-env-.conda-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
