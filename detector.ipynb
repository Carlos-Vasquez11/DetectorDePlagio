{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c9746c2-55d9-4d2d-b651-88072255591f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.training.example import Example\n",
    "from PyPDF2 import PdfReader\n",
    "from pptx import Presentation\n",
    "from docx import Document\n",
    "from collections import Counter\n",
    "person_tagger = spacy.load(\"es_core_news_sm\")\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "\n",
    "#Para detector de plagio\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "#Para web:\n",
    "from googlesearch import search\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import fnmatch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb0fcdd7-62c7-4900-873d-5d99a327568f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#.docx .pptx .pdf\n",
    "def obtener_nombre_archivos(extension):\n",
    "    archivos = []\n",
    "    for archivo in os.listdir(\"./dataset\"):\n",
    "        if archivo.endswith(extension):\n",
    "            archivos.append(archivo)\n",
    "    return archivos\n",
    "\n",
    "def leer_pdf(ruta_archivo):\n",
    "    archivoLectura = open(ruta_archivo,'rb')\n",
    "    documento = PdfReader(archivoLectura)\n",
    "    texto_documento = \"\"\n",
    "\n",
    "    for pagina in documento.pages:\n",
    "        texto_documento += pagina.extract_text()\n",
    "\n",
    "    texto_limpio = re.sub(r'[^a-zA-Z0-9.,áéíóúÁÉÍÓÚñÑ]+', ' ', texto_documento)\n",
    "    return texto_limpio\n",
    "\n",
    "def leer_pptx(filepath):\n",
    "    presentation = Presentation(filepath)\n",
    "    texto_documento = \"\"\n",
    "\n",
    "    # Recorrer todas las diapositivas\n",
    "    for slide in presentation.slides:\n",
    "        # Recorrer todos los elementos de texto en la diapositiva\n",
    "        for shape in slide.shapes:\n",
    "            if shape.has_text_frame:\n",
    "                for paragraph in shape.text_frame.paragraphs:\n",
    "                    for run in paragraph.runs:\n",
    "                        texto_documento += run.text\n",
    "                        texto_documento += \" \"\n",
    "                        \n",
    "    texto_limpio = re.sub(r'[^a-zA-Z0-9.,áéíóúÁÉÍÓÚñÑ]+', ' ', texto_documento)\n",
    "    return texto_limpio\n",
    "\n",
    "def leer_docx(ruta_archivo):\n",
    "    doc = Document(ruta_archivo)\n",
    "    texto_documento = \"\"\n",
    "\n",
    "    # Recorrer todos los párrafos del documento\n",
    "    for paragraph in doc.paragraphs:\n",
    "        texto_documento += paragraph.text\n",
    "        texto_documento += \" \"\n",
    "\n",
    "    texto_limpio = re.sub(r'[^a-zA-Z0-9.,áéíóúÁÉÍÓÚñÑ]+', ' ', texto_documento)\n",
    "    return texto_limpio\n",
    "\n",
    "def leer_web(link):\n",
    "    try:\n",
    "        pagina = requests.get(link).text\n",
    "        soup = BeautifulSoup(pagina, 'html.parser')\n",
    "        buscarContenido = soup.find('body')\n",
    "        \n",
    "        if buscarContenido:\n",
    "            parrafos = buscarContenido.find_all('p')\n",
    "            listaStr = []\n",
    "            \n",
    "            for elemento in parrafos:\n",
    "                parrafo = elemento.get_text(strip=True)\n",
    "                texto_limpio = re.sub(r'[^a-zA-Z0-9.,áéíóúÁÉÍÓÚñÑ\\s]+', ' ', parrafo)\n",
    "                listaStr.append(texto_limpio)\n",
    "            \n",
    "            texto_concatenado = ' '.join(listaStr)\n",
    "            return texto_concatenado\n",
    "        else:\n",
    "            print(\"No se encontró contenido en la página.\")\n",
    "            return \"\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error al acceder a la página:\", e)\n",
    "        return \"\"\n",
    "\n",
    "def leer_documentos(nombre_archivos):\n",
    "    archivos = []\n",
    "    for archivo in nombre_archivos:\n",
    "        if(archivo.endswith(\".pdf\")):\n",
    "            archivos.append(leer_pdf(\"./dataset/\" + archivo))\n",
    "        if(archivo.endswith(\".pptx\")):\n",
    "            archivos.append(leer_pptx(\"./dataset/\" + archivo))\n",
    "        if(archivo.endswith(\".docx\")):\n",
    "            archivos.append(leer_docx(\"./dataset/\" + archivo))\n",
    "    return archivos\n",
    "\n",
    "def leer_archivo_analizar(nombre_archivo):\n",
    "    if(nombre_archivo.endswith(\".pdf\")):\n",
    "            return leer_pdf(nombre_archivo)\n",
    "    if(nombre_archivo.endswith(\".pptx\")):\n",
    "            return leer_pptx(nombre_archivo)\n",
    "    if(nombre_archivo.endswith(\".docx\")):\n",
    "            return leer_docx(nombre_archivo)\n",
    "    if(nombre_archivo.startswith(\"https://\")):\n",
    "            return leer_web(nombre_archivo)\n",
    "\n",
    "def obtener_nombre_archivo(ruta):\n",
    "    if(ruta.startswith(\"https://\")):\n",
    "        return ruta\n",
    "    nombre_archivo = os.path.basename(ruta)\n",
    "    return nombre_archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13065889-f941-4b82-8f9a-cd40c2722ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecución\n",
    "nombres_archivos_pdf = obtener_nombre_archivos(\".pdf\")\n",
    "nombres_archivos_docx = obtener_nombre_archivos(\".docx\")\n",
    "nombres_archivos_doc = obtener_nombre_archivos(\".doc\")\n",
    "nombres_archivos_pptx = obtener_nombre_archivos(\".pptx\")\n",
    "nombre_archivos = []\n",
    "nombre_archivos.extend(nombres_archivos_pdf)\n",
    "nombre_archivos.extend(nombres_archivos_docx)\n",
    "nombre_archivos.extend(nombres_archivos_doc)\n",
    "nombre_archivos.extend(nombres_archivos_pptx)\n",
    "corpus = leer_documentos(nombre_archivos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e94214e-ea20-4119-ba62-ae8c3c1afa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre del Archivo Procesado: Economía de experiencia.pdf\n"
     ]
    }
   ],
   "source": [
    "#nombre_archivo_a_analizar = \"https://carlos-vasquez11.github.io/pagina-nlp/\"\n",
    "nombre_archivo_a_analizar = \"./Economía de experiencia.pdf\"\n",
    "documento_a_analizar = leer_archivo_analizar(nombre_archivo_a_analizar)\n",
    "documento_a_analizar_clean = re.sub(r'\\W+', ' ', documento_a_analizar)\n",
    "print(\"Nombre del Archivo Procesado: \" + obtener_nombre_archivo(nombre_archivo_a_analizar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30494bb3-093e-42c2-9773-d302b5a0907e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities [('Alumno Pedro', 'MISC')]\n"
     ]
    }
   ],
   "source": [
    "#Entrenamiento\n",
    "entrenamiento = [\n",
    "    (\"Alumno Pedro picazo\",{\"entities\": [(0,6,\"IDENTIFICADOR\")]})\n",
    "]\n",
    "\n",
    "ner = person_tagger.get_pipe(\"ner\")\n",
    "\n",
    "for _, anotaciones in entrenamiento:\n",
    "    for ent in anotaciones.get(\"entities\"):\n",
    "        ner.add_label(ent[2])\n",
    "\n",
    "#disable_pipes = [pipe for pipe in person_tagger.pipe_names if pipe != 'ner']\n",
    "#with person_tagger.disable_pipes(*disable_pipes):\n",
    "optimizer = person_tagger.resume_training()\n",
    "    \n",
    "for iteration in range(5):\n",
    "       random.shuffle(entrenamiento)\n",
    "       losses = {}\n",
    "        \n",
    "       batches = minibatch(entrenamiento,size=compounding(3.8,4.0,1.001))\n",
    "       for batch in batches:\n",
    "            examples = []\n",
    "            for text, annotation in batch:\n",
    "                examples.append(Example.from_dict(person_tagger.make_doc(text), annotation))\n",
    "            person_tagger.update(examples, drop=0.5, losses=losses, sgd=optimizer)\n",
    "\n",
    "#Ing Hernán Borré Estudiante Juanquin Perez | MISC o PER\n",
    "            \n",
    "for texto, _ in entrenamiento:\n",
    "    documento = person_tagger(texto)\n",
    "    print('Entities', [(ent.text, ent.label_) for ent in documento.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d05dd4e-ee35-4f44-bc65-d7e57b0a2271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posibles nombres del Autor:\n",
      " Gallazzi Pablo Gabriel\n"
     ]
    }
   ],
   "source": [
    "print(\"Posibles nombres del Autor:\")\n",
    "tokens = word_tokenize(documento_a_analizar_clean)\n",
    "tokens_documento = [token.lower() for token in tokens if token.lower() not in stop_words]\n",
    "per_tagger = person_tagger(documento_a_analizar_clean)\n",
    "\n",
    "def contains_word(text, word):\n",
    "    return word in text\n",
    "\n",
    "for autor in per_tagger.ents[0:12]:\n",
    "    token_aux = word_tokenize(autor.text)\n",
    "    if(autor.label_ == \"PER\" and len(token_aux) > 2):\n",
    "        if(contains_word(autor.text,\"Alumno\")):\n",
    "            index = autor.text.find(\"Alumno\")\n",
    "            nuevo_texto = autor.text[index:]\n",
    "            nuevo_texto = nuevo_texto.replace(\"Alumno\",\"\")\n",
    "            print(nuevo_texto)\n",
    "        else:\n",
    "            print(autor.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89953490-0ccf-4b67-8d6d-d96f262b9a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "márketing en internet y nueva economía economía de experiencia\n"
     ]
    }
   ],
   "source": [
    "def eliminar_ultima_palabra(cadena):\n",
    "    palabras = cadena.split()\n",
    "    palabras_sin_ultima = palabras[:-1]\n",
    "    nueva_cadena = \" \".join(palabras_sin_ultima)\n",
    "    return nueva_cadena\n",
    "\n",
    "def obtener_tema_documento(text, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "    nlp = spacy.load(\"es_core_news_sm\", disable=[\"parser\", \"ner\"])\n",
    "    doc = nlp(text)\n",
    "    new_text = []\n",
    "    propn = 0\n",
    "    final = \"\"\n",
    "    final_verdadero = \"\"\n",
    "    while len(word_tokenize(final_verdadero)) < 2:\n",
    "        for token in doc:\n",
    "            if token.pos_ == \"PROPN\":\n",
    "                propn += 1\n",
    "            else:\n",
    "                propn = 0\n",
    "            if(propn == 2):\n",
    "                break;\n",
    "            new_text.append(token.lower_)\n",
    "        final = \" \".join(new_text)\n",
    "        final_verdadero = eliminar_ultima_palabra(final)\n",
    "    \n",
    "    return final_verdadero\n",
    "\n",
    "temaPrincipal = \"\"\n",
    "\n",
    "temaPrincipal = obtener_tema_documento(documento_a_analizar)\n",
    "\n",
    "print(temaPrincipal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b83854ee-e81d-4dfa-96b5-8fb83b1fc26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temas principales:\n",
      "experiencia\n",
      "personalización\n",
      "producto\n",
      "economía\n",
      "dimensión\n",
      "ejemplo\n",
      "masivo\n",
      "empresa\n",
      "hablar\n",
      "referir\n"
     ]
    }
   ],
   "source": [
    "#Investigar sobre LDA\n",
    "def lematizacion(text, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "    nlp = spacy.load(\"es_core_news_sm\", disable=[\"parser\", \"ner\"])\n",
    "    doc = nlp(text)\n",
    "    new_text = []\n",
    "    for token in doc:\n",
    "        if token.pos_ in allowed_postags:\n",
    "            new_text.append(token.lemma_)\n",
    "    final = \" \".join(new_text)\n",
    "    return final\n",
    "\n",
    "def identificar_tema(texto):\n",
    "    # Procesar el texto con spaCy\n",
    "    doc = person_tagger(texto)\n",
    "\n",
    "    # Filtrar palabras irrelevantes, puntuaciones y números\n",
    "    palabras = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "    palabras = [re.sub(r'[0-9]', '', palabra) for palabra in palabras]\n",
    "\n",
    "    # Contar la frecuencia de las palabras\n",
    "    frecuencias = Counter(palabras)\n",
    "\n",
    "    # Obtengo las 11 palabras más frecuentes\n",
    "    temas_principales = [tema for tema, frecuencia in frecuencias.most_common(11)]\n",
    "\n",
    "    #La primera posición siempre es el espacio, es mejor no retornarlo\n",
    "    return temas_principales[1:]\n",
    "\n",
    "texo_lematizado = lematizacion(documento_a_analizar)\n",
    "temas = identificar_tema(texo_lematizado)\n",
    "print(\"Temas principales:\")\n",
    "for tema in temas:\n",
    "    print(tema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a78231c-0401-4c5d-99ee-9ae7f60a4d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nquery = temaPrincipal\\n#Por defecto nos retorna los primeros 10 resultados\\nfor link in search(query,num_results=10):\\n    print(link)\\n    pagina = leer_web(link)\\n    corpus.append(pagina)\\n    nombre_archivos.append(link)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Agrego fuentes de internet al corpus\n",
    "\"\"\"\n",
    "query = temaPrincipal\n",
    "#Por defecto nos retorna los primeros 10 resultados\n",
    "for link in search(query,num_results=10):\n",
    "    time.sleep(1)\n",
    "    print(link)\n",
    "    pagina = leer_web(link)\n",
    "    corpus.append(pagina)\n",
    "    nombre_archivos.append(link)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc355981-0c25-4b9e-b03c-6ed6ced68cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documento analizado: Tp3-MKT-Romano Karra.docx\n",
      "Porcentaje de potencial plagio: 21.97%\n",
      "Frase: ' Márketing en internet y nueva economía Economía de experiencia Profesores Dr. Alejandro Prince Ing.', Ubicación: 0\n",
      "PFPDC:   Márketing en internet y nueva economía TP N. 3 Profesores Dr. Alejandro Prince Ing.\n",
      "Frase: 'Qué 3 elementos hacen resurgir con fuerza la idea de una economía de experiencia 2.', Ubicación: 3\n",
      "PFPDC:  Explique y grafique las dimensiones y campos de la experiencia Describa impresiones y sus distintas dimensiones.\n",
      "Frase: 'Diferencias con Producto y Servicio.', Ubicación: 5\n",
      "PFPDC:  Explique y grafique las dimensiones y campos de la experiencia Describa impresiones y sus distintas dimensiones.\n",
      "Frase: 'Explique y grafique las dimensiones y campos de la experiencia.', Ubicación: 7\n",
      "PFPDC:  De 3 ejemplos distintos reales si conoce, o invente de experiencias con estimulación de los sentidos.\n",
      "Frase: 'Describa impresiones y sus distintas dimensiones.', Ubicación: 9\n",
      "PFPDC:  De 3 ejemplos distintos reales si conoce, o invente de experiencias con estimulación de los sentidos.\n",
      "Frase: 'De 3 ejemplos distintos reales si conoce, o invente de experiencias con estimulación de los sentidos.', Ubicación: 11\n",
      "PFPDC:  Qué es la personalización masiva Explique la progresión del valor.\n",
      "Frase: 'Qué es la personalización masiva Explique la progresión del valor.', Ubicación: 13\n",
      "PFPDC:  Cuáles son las ventajas para la empresa de la personalización masiva Describa los 4 tipos de personalización masiva.\n",
      "Frase: 'Cuáles son las ventajas para la empresa de la personalización masiva 8.', Ubicación: 15\n",
      "PFPDC:  8 Describa los 4 tipos de personalización masiva.\n",
      "Frase: 'Describa los 4 tipos de personalización masiva.', Ubicación: 16\n",
      "No se encontró la línea correspondiente.\n",
      "Frase: 'Qué aporta el ciberespacio al tema sacrificio del cliente Respuestas 1.', Ubicación: 18\n",
      "PFPDC:  Qué aporta el ciberespacio al tema sacrificio del cliente 1 En primer lugar, al poder de la tecnología que facilita el como darle a los clientes una grata experiencia.\n",
      "Frase: 'La creciente intensidad de la competencia que hace que se tengan que diferenciar.', Ubicación: 20\n",
      "PFPDC:  En segundo lugar, la intensidad de la competencia, las empresas buscan la diferenciación con la mejora de la experiencia.\n",
      "Frase: 'La prosperidad, es decir, hoy en día se busca menos rutina, más sorpresas experiencias 2.', Ubicación: 21\n",
      "PFPDC:  Por ultimo la prosperidad, ya que el hombre busca mas celebraciones, menos rutina y más sorpresas.\n",
      "Frase: 'Cuando hablamos del producto nos referimos a, por ejemplo, una taza, un café, etc Si nos referimos a un servicio hablamos de un conjunto de actividades es decir algo customizado para el cliente, pero cuando hablamos de una experiencia nos referimos a algo más, pagamos para disfrutar de una serie de eventos acondicionados para atraparnos y gozar de una manera distinta que solo comprar el producto o adquirir el servicio.', Ubicación: 22\n",
      "No se encontró la línea correspondiente.\n",
      "Frase: 'De hecho se usan los productos y servicios como herramientas para lograr entretener a los clientes.', Ubicación: 23\n",
      "No se encontró la línea correspondiente.\n",
      "Frase: 'Dimensiones Grado de participación que puede ser pasiva el cliente es un observador o activa tiene como protagonista al cliente Conexión cliente evento que puede ir desde la atención o absorción hasta la inmersión física o virtual.', Ubicación: 25\n",
      "PFPDC:  2 La experiencia es la cuarta forma de acercarse al mercado y consiste en ofrecerle al cliente sensaciones y emociones con un producto, un commodity o un servicio.\n",
      "Frase: 'Estas dimensiones se cruzan generando los cuatro campos Entretenimiento La persona pasivamente absorbe lo que lo que ocurre a través de sus sentidos.', Ubicación: 26\n",
      "PFPDC:  De 3 ejemplos distintos reales si conoce, o invente de experiencias con estimulación de los sentidos.\n",
      "Frase: 'Educación Alcanza con la atención por parte de la persona, pero requiere participación activa de la misma.', Ubicación: 27\n",
      "PFPDC:  7 Cuáles son las ventajas para la empresa de la personalización masiva Mejores precios, menor necesidad de descuentos, mayores ingresos por cliente, más clientes con menor costo de adquisición y niveles de retención más altos.\n",
      "Frase: 'Estética El individuo se sumerge pero no participa.', Ubicación: 28\n",
      "PFPDC:  Están los siguientes tipos de personalización Colaborativa la empresa interactúa con el cliente para saber qué quiere, y luego lo produce.\n",
      "Frase: 'Escapismo Máxima inmersión y protagonismo absoluto.', Ubicación: 29\n",
      "PFPDC:  Transparente el cliente recibe una oferta hecha a medida para él sin enterarse, lo cual permite que el mismo no tenga que realizar el proceso de construcción del producto todas las veces.\n",
      "Frase: 'Cuando nos referimos a impresiones, hablamos de lo que se lleva el cliente de la experiencia vivida, es decir, los recuerdos.', Ubicación: 31\n",
      "No se encontró la línea correspondiente.\n",
      "Frase: 'Sofisticación Nivel de refinamiento o lujo.', Ubicación: 35\n",
      "No se encontró la línea correspondiente.\n",
      "Frase: 'La personalización masiva, se trata de atender a los clientes de una forma única, combinando los imperativos vigentes de bajo costo y la individualización que requiere el mercado.', Ubicación: 40\n",
      "PFPDC:  Transparente el cliente recibe una oferta hecha a medida para él sin enterarse, lo cual permite que el mismo no tenga que realizar el proceso de construcción del producto todas las veces.\n",
      "Frase: 'Las ventajas para la empresa son varias, mejorar los precios, mayor ingreso por cliente, menor necesidad de hacer descuentos, más retención, y adquisición de clientes por un costo más bajo.', Ubicación: 49\n",
      "No se encontró la línea correspondiente.\n",
      "Frase: 'Los cuatro tipos son Personalización colaborativa o experiencia exploratoria La empresa interactúa directamente con el cliente, el producto final surge del trabajo conjunto.', Ubicación: 51\n",
      "No se encontró la línea correspondiente.\n",
      "Frase: 'Personalización transparente o experiencia elusiva El cliente ya recibe una oferta a medida y no se entera explícitamente del proceso de personalización 9.', Ubicación: 55\n",
      "No se encontró la línea correspondiente.\n",
      " \n",
      "Porcentaje de plagio total aproximado: 31.38%\n"
     ]
    }
   ],
   "source": [
    "def parrafos_potencialmente_plagiados(documents, new_document):\n",
    "    model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
    "    sentences = nltk.sent_tokenize(new_document)\n",
    "    sentencesD = nltk.sent_tokenize(documents[0])\n",
    "\n",
    "    # Embeddings de los documentos existentes\n",
    "    document_embeddings = model.encode(sentencesD, convert_to_tensor=True)\n",
    "\n",
    "    # Embeddings de las frases del nuevo documento\n",
    "    new_embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "    # Calcular la similitud de coseno entre los embeddings\n",
    "    cos_sim_scores = util.pytorch_cos_sim(new_embeddings, document_embeddings)\n",
    "\n",
    "    potential_plagiarized_sentences = []\n",
    "    plagiarized_lines = []  # Lista para almacenar las líneas plagiadas del documento\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        max_sim_score = max(cos_sim_scores[i])\n",
    "        if max_sim_score > 0.7:  # Ajusta el umbral según sea necesario\n",
    "            doc_index = cos_sim_scores[i].argmax().item()\n",
    "            plagiarism_percentage = max_sim_score * 100\n",
    "            potential_plagiarized_sentences.append((sentence, doc_index, i, plagiarism_percentage))\n",
    "\n",
    "            if doc_index < len(sentencesD):\n",
    "                plagiarized_lines.append(sentencesD[doc_index])  # Agregar la línea plagiada del documento\n",
    "            else:\n",
    "                plagiarized_lines.append(\"\")  # Agregar una cadena vacía si el índice está fuera de rango\n",
    "\n",
    "    return potential_plagiarized_sentences, plagiarized_lines\n",
    "\n",
    "frases_plagiadas = set()\n",
    "\n",
    "for index in range(len(corpus)):\n",
    "    similitud = SequenceMatcher(None, documento_a_analizar, corpus[index]).ratio() * 100\n",
    "    if similitud > 20:\n",
    "        print(\"Documento analizado: \" + nombre_archivos[index])\n",
    "        print(f\"Porcentaje de potencial plagio: {similitud:.2f}%\")\n",
    "        potential_plagiarized_sentences, plagiarized_lines = parrafos_potencialmente_plagiados([corpus[index]], documento_a_analizar)\n",
    "        for sentence, doc_index, sent_index, plagiarism_percentage in potential_plagiarized_sentences:\n",
    "            print(f\"Frase: '{sentence}', Ubicación: {sent_index}\")\n",
    "            if doc_index < len(plagiarized_lines):\n",
    "                print(f\"PFPDC:  {plagiarized_lines[doc_index]}\")\n",
    "                frases_plagiadas.add(sent_index)\n",
    "            else:\n",
    "                print(\"No se encontró la línea correspondiente.\")\n",
    "        print(\" \")\n",
    "        \n",
    "##\n",
    "\n",
    "def contar_caracteres(lista_cadenas):\n",
    "    total_caracteres = 0\n",
    "    for cadena in lista_cadenas:\n",
    "        total_caracteres += len(cadena)\n",
    "    return total_caracteres\n",
    "\n",
    "def contar_caracteres_por_posicion(lista_cadenas, lista_indices):\n",
    "    total_caracteres = 0\n",
    "    for indice in lista_indices:\n",
    "        if indice < len(lista_cadenas):\n",
    "            total_caracteres += len(lista_cadenas[indice])\n",
    "    return total_caracteres\n",
    "        \n",
    "total_frases = nltk.sent_tokenize(documento_a_analizar)\n",
    "caracteres_totales = contar_caracteres(total_frases)\n",
    "caracteres_plagiados = contar_caracteres_por_posicion(total_frases,frases_plagiadas)\n",
    "porcentaje_plagio = caracteres_plagiados / caracteres_totales * 100\n",
    "print(f\"Porcentaje de plagio total aproximado: {porcentaje_plagio:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a722e65-e9af-4891-9d6c-7c803efa476a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Disclaimer: Este trabajo fue desarrollado en base a lo aprendido en la cursada de la materia de Procesamiento de Lenguaje Natural de la UTN frba,\\n    la misma fue dicatada por el profesor Mg. Ing. Hernán Borré (https://github.com/hernanborre)\\n    \\n    Fuentes:\\n    (2023). Programación en Python. ChatGPT\\n    https://chat.openai.com/\\n    \\n    AiEgineering. (2021). Custom Named Entity Recognition using Python\\n    https://www.youtube.com/watch?v=1ePkOSGoIFI&t=855s&ab_channel=AIEngineering\\n    \\n    codebasics. (2021). Named Entity Recognition (NER): NLP Tutorial For Beginners - 12. Youtube.\\n    https://www.youtube.com/watch?v=2XUhKpH0p4M&list=PL6qsQ_bxK9ZznUs0WiDGhtOr6A_EAHWKS&index=2&t=1087s&ab_channel=codebasics\\n    \\n    fpread. (2020). scrapear para detectar plagios python. Youtube.\\n    https://www.youtube.com/watch?v=OzgQ0QYxO20&list=PL6qsQ_bxK9ZznUs0WiDGhtOr6A_EAHWKS&index=4&ab_channel=fpred\\n    \\n    I know python (2021). Plagiarism detector using python. Youtube.\\n    https://www.youtube.com/watch?v=lRzC3w2NDg0&list=PL6qsQ_bxK9ZznUs0WiDGhtOr6A_EAHWKS&index=1&ab_channel=Iknowpython\\n    \\n    (2023). collections — Container datatypes. Python Org.\\n    https://docs.python.org/3/library/collections.html#collections.Counter\\n    \\n    (2021). Python + Google: BÚSQUEDAS AUTOMATIZADAS: Cómo realizar búsquedas en Google usando Python. Youtube\\n    https://www.youtube.com/watch?v=FE-WeaNRAeI&ab_channel=TruzzBlogg\\n    \\n    (2023). googlesearch. pypi.org\\n    https://pypi.org/project/googlesearch-python/\\n    \\n    (2023). spaCy.\\n    https://spacy.io/\\n    \\n    (2023). PyPDF2. pypi.org\\n    https://pypi.org/project/PyPDF2/\\n    \\n    (2023). docx.\\n    https://python-docx.readthedocs.io/en/latest/\\n    \\n    (2023). pptx.\\n    https://python-pptx.readthedocs.io/en/latest/\\n    \\n    \\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Disclaimer: Este trabajo fue desarrollado en base a lo aprendido en la cursada de la materia de Procesamiento de Lenguaje Natural de la UTN frba,\n",
    "    la misma fue dicatada por el profesor Mg. Ing. Hernán Borré (https://github.com/hernanborre)\n",
    "    \n",
    "    Fuentes:\n",
    "    (2023). Programación en Python. ChatGPT\n",
    "    https://chat.openai.com/\n",
    "    \n",
    "    AiEgineering. (2021). Custom Named Entity Recognition using Python\n",
    "    https://www.youtube.com/watch?v=1ePkOSGoIFI&t=855s&ab_channel=AIEngineering\n",
    "    \n",
    "    codebasics. (2021). Named Entity Recognition (NER): NLP Tutorial For Beginners - 12. Youtube.\n",
    "    https://www.youtube.com/watch?v=2XUhKpH0p4M&list=PL6qsQ_bxK9ZznUs0WiDGhtOr6A_EAHWKS&index=2&t=1087s&ab_channel=codebasics\n",
    "    \n",
    "    fpread. (2020). scrapear para detectar plagios python. Youtube.\n",
    "    https://www.youtube.com/watch?v=OzgQ0QYxO20&list=PL6qsQ_bxK9ZznUs0WiDGhtOr6A_EAHWKS&index=4&ab_channel=fpred\n",
    "    \n",
    "    I know python (2021). Plagiarism detector using python. Youtube.\n",
    "    https://www.youtube.com/watch?v=lRzC3w2NDg0&list=PL6qsQ_bxK9ZznUs0WiDGhtOr6A_EAHWKS&index=1&ab_channel=Iknowpython\n",
    "    \n",
    "    (2023). collections — Container datatypes. Python Org.\n",
    "    https://docs.python.org/3/library/collections.html#collections.Counter\n",
    "    \n",
    "    (2021). Python + Google: BÚSQUEDAS AUTOMATIZADAS: Cómo realizar búsquedas en Google usando Python. Youtube\n",
    "    https://www.youtube.com/watch?v=FE-WeaNRAeI&ab_channel=TruzzBlogg\n",
    "    \n",
    "    (2023). googlesearch. pypi.org\n",
    "    https://pypi.org/project/googlesearch-python/\n",
    "    \n",
    "    (2023). spaCy.\n",
    "    https://spacy.io/\n",
    "    \n",
    "    (2023). PyPDF2. pypi.org\n",
    "    https://pypi.org/project/PyPDF2/\n",
    "    \n",
    "    (2023). docx.\n",
    "    https://python-docx.readthedocs.io/en/latest/\n",
    "    \n",
    "    (2023). pptx.\n",
    "    https://python-pptx.readthedocs.io/en/latest/\n",
    "    \n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72f6228-6280-46b6-b06c-d0ac0babc8b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-nlp] *",
   "language": "python",
   "name": "conda-env-.conda-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
